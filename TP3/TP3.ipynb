{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a159ae7",
   "metadata": {},
   "source": [
    "# CNN Avec peu de données\n",
    "\n",
    "#### Chargement du code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7400fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(123)  # for reproducibility\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP3_utils import load_dataset\n",
    "\n",
    "DATA_PATH = 'Data'\n",
    "CATEGORIES = [\"accordion\", \"anchor\", \"barrel\", \"binocular\"]\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test), num_classes = load_dataset(DATA_PATH, CATEGORIES)\n",
    "data_shape = X_train[0].shape\n",
    "\n",
    "print(\"Training data shape:\", data_shape)\n",
    "print(\"Training labels shape:\", Y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", Y_test.shape)\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eeed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from TP3_utils import CNN, affiche, eval_classif\n",
    "# from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# lr_schedulerBis = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "# earlStopBis = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, restore_best_weights=True, verbose = 1, mode=\"min\", start_from_epoch=20)\n",
    "\n",
    "lr=1e-4\n",
    "batch_size=min([X_train.shape[0], 256])\n",
    "epochs=16\n",
    "ad= Adam(learning_rate=lr)\n",
    "\n",
    "model = CNN(data_shape)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=ad,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tps1 = time.time()\n",
    "history =model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test)\n",
    "#     callbacks=[earlStopBis, lr_schedulerBis],\n",
    ")\n",
    "tps2 = time.time()\n",
    "\n",
    "# Evaluation\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "affiche(history)\n",
    "preds = model.predict(X_test)\n",
    "eval_classif(Y_test, preds)\n",
    "print(\"Temps d'entraînement : {:.2f} secondes\".format(tps2 - tps1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP3_utils import data_augmentation\n",
    "\n",
    "inputs = tf.keras.Input(shape=data_shape)\n",
    "x = data_augmentation(inputs)\n",
    "x = CNN(data_shape)(x)\n",
    "model_aug = tf.keras.Model(inputs, x, name=\"CNN_with_Augmentation\")\n",
    "\n",
    "model_aug.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entraînement\n",
    "history =model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test)\n",
    "#     callbacks=[earlStopBis, lr_schedulerBis],\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "affiche(history)\n",
    "preds = model.predict(X_test)\n",
    "eval_classif(Y_test, preds)\n",
    "print(\"Temps d'entraînement : {:.2f} secondes\".format(tps2 - tps1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from TP3_utils import MLP_transfer\n",
    "\n",
    "# Transfer learning\n",
    "input_tensor = Input(shape=(224,224,3))\n",
    "VGG = tf.keras.applications.VGG16(weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_tensor=input_tensor\n",
    ")\n",
    "for layer in VGG.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_transfer = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=VGG.output_shape[1:]),\n",
    "    MLP_transfer(VGG.output_shape[1:], num_classes)\n",
    "])\n",
    "model_transfer.summary()\n",
    "\n",
    "model_transfer.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tps1 = time.time()\n",
    "history =model_transfer.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test)\n",
    "#     callbacks=[earlStopBis, lr_schedulerBis],\n",
    ")\n",
    "tps2 = time.time()\n",
    "\n",
    "################## Fine-tuning #################\n",
    "\n",
    "for layer in VGG.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_transfer.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(1e-6),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tps1 = time.time()\n",
    "history =model_transfer.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test)\n",
    "#     callbacks=[earlStopBis, lr_schedulerBis],\n",
    ")\n",
    "tps2 = time.time()\n",
    "\n",
    "loss, accuracy = model_transfer.evaluate(X_test, Y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "affiche(history)\n",
    "preds = model_transfer.predict(X_test)\n",
    "eval_classif(Y_test, preds)\n",
    "print(\"Temps d'entraînement : {:.2f} secondes\".format(tps2 - tps1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c00e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_mnist_with_noise' from 'TP3_utils' (/Users/joachim/gws/TPs_Rob/TP_ML/TP3/TP3_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mTP3_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_mnist_with_noise\n\u001b[32m      2\u001b[39m (X_train, Y_train, X_train_noisy), (X_test, Y_test, X_test_noisy) = load_mnist_with_noise()\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_mnist_with_noise' from 'TP3_utils' (/Users/joachim/gws/TPs_Rob/TP_ML/TP3/TP3_utils.py)"
     ]
    }
   ],
   "source": [
    "from TP3_utils import load_mnist_with_noise\n",
    "(X_train, Y_train, X_train_noise), (X_test, Y_test, X_test_noise) = load_mnist_with_noise()\n",
    "\n",
    "# Display the train data and a version of it with added noise\n",
    "for i in range(5):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(X_train[i,:].reshape([28,28]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.subplot(2,5,i+6)\n",
    "    plt.imshow(X_train_noise[i,:].reshape([28,28]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0df6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP3_utils import auto_encoder\n",
    "\n",
    "model_ae = auto_encoder(data_shape)\n",
    "\n",
    "model_ae.compile(\n",
    "    loss=\"mean_square_error\",\n",
    "    optimizer=ad,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history =model_ae.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, Y_test)\n",
    "#     callbacks=[earlStopBis, lr_schedulerBis],\n",
    ")\n",
    "\n",
    "loss, accuracy = model_ae.evaluate(X_test, Y_test)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n",
    "\n",
    "affiche(history)\n",
    "preds = model_ae.predict(X_test)\n",
    "eval_classif(Y_test, preds)\n",
    "print(\"Temps d'entraînement : {:.2f} secondes\".format(tps2 - tps1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8821a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TP3_utils import vae, loss_func\n",
    "\n",
    "model_vae, mu, log_variance = vae(data_shape)\n",
    "\n",
    "model_vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss=loss_func(mu, log_variance))\n",
    "model_vae.fit(X_train, X_train, epochs=70, batch_size=32, shuffle=True, validation_data=(X_test, X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
